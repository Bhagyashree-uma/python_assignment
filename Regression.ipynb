{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nMzZzxTsaFf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Simple Linear Regression?\n",
        "Simple Linear Regression is a statistical method used to model the relationship between two continuous variables by fitting a linear equation to observed data. The equation takes the form ( Y = mX + c ), where ( Y ) is the dependent variable, ( X ) is the independent variable, ( m ) is the slope, and ( c ) is the intercept.\n",
        "\n",
        "2.What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Linearity: The relationship between the independent and dependent variable is linear.\n",
        "Independence: The residuals (errors) are independent.\n",
        "Homoscedasticity: The residuals have constant variance at all levels of the independent variable.\n",
        "Normality: The residuals are normally distributed.\n",
        "\n",
        "3.What does the coefficient ( m ) represent in the equation ( Y = mX + c )?\n",
        " The coefficient ( m ) represents the slope of the regression line, indicating the change in the dependent variable ( Y ) for a one-unit increase in the independent variable ( X ).\n",
        "\n",
        "4.What does the intercept ( c ) represent in the equation ( Y = mX + c )?\n",
        "The intercept ( c ) represents the expected value of ( Y ) when ( X ) is zero. It is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "5.How do we calculate the slope ( m ) in Simple Linear Regression?  \n",
        "The slope ( m ) can be calculated using the formula: [ m = \\frac{N(\\sum XY) - (\\sum X)(\\sum Y)}{N(\\sum X^2) - (\\sum X)^2} ] where ( N ) is the number of data points, ( \\sum XY ) is the sum of the product of ( X ) and ( Y ), ( \\sum X ) is the sum of ( X ), and ( \\sum Y ) is the sum of ( Y ).\n",
        "\n",
        "6.What is the purpose of the least squares method in Simple Linear Regression? The least squares method aims to minimize the sum of the squared differences (residuals) between the observed values and the values predicted by the linear model. This results in the best-fitting line through the data points.\n",
        "\n",
        "7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "The coefficient of determination ( R² ) indicates the proportion of the variance in the dependent variable that can be explained by the independent variable. An ( R² ) value of 0 means no explanatory power, while a value of 1 means perfect explanatory power.\n",
        "\n",
        "Multiple Linear Regression\n",
        "8.What is Multiple Linear Regression?\n",
        " Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "9.What is the main difference between Simple and Multiple Linear Regression?\n",
        " The main difference is that Simple Linear Regression involves one independent variable, while Multiple Linear Regression involves multiple independent variables.\n",
        "\n",
        "10.What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Linearity: The relationship between the independent variables and the dependent variable is linear.\n",
        "Independence: The residuals are independent.\n",
        "Homoscedasticity: The residuals have constant variance.\n",
        "Normality: The residuals are normally distributed.\n",
        "No multicollinearity: Independent variables should not be too highly correlated with each other.\n",
        "\n",
        "11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Heteroscedasticity refers to the condition where the variance of the residuals is not constant across all levels of the independent variables. It can lead to inefficient estimates and affect the validity of hypothesis tests.\n",
        "\n",
        "12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Remove highly correlated predictors.\n",
        "Combine correlated variables into a single predictor (e.g., using PCA).\n",
        "Use regularization techniques like Ridge or Lasso regression.\n",
        "\n",
        "13.What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "One-hot encoding: Creating binary variables for each category.\n",
        "Label encoding: Assigning a unique integer to each category.\n",
        "Dummy variables: Similar to one-hot encoding but omitting one category to avoid multicollinearity.\n",
        "\n",
        "14.What is the role of interaction terms in Multiple Linear Regression? Interaction terms are used to model the effect of two or more independent variables on the dependent variable when the effect of one variable depends on the level of another variable.\n",
        "\n",
        "15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        " In Simple Linear Regression, the intercept represents the expected value of ( Y ) when ( X ) is zero. In Multiple Linear Regression, the intercept represents the expected value of the dependent variable when all independent variables are equal to zero, which may not always be a meaningful scenario if the independent variables cannot realistically take on a value of zero.\n",
        "\n",
        "16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "The slope indicates the strength and direction of the relationship between an independent variable and the dependent variable. A positive slope suggests that as the independent variable increases, the dependent variable also increases, while a negative slope indicates the opposite. This affects predictions by determining how much the dependent variable is expected to change with a unit change in the independent variable.\n",
        "\n",
        "17.How does the intercept in a regression model provide context for the relationship between variables?\n",
        "The intercept provides a baseline value for the dependent variable when all independent variables are at their reference levels. It helps in understanding the starting point of the dependent variable in the context of the model.\n",
        "\n",
        "18.What are the limitations of using R² as a sole measure of model performance?\n",
        " R² does not account for the number of predictors in the model, which can lead to overfitting. It also does not indicate whether the independent variables are a good fit for the model or whether the model is appropriate for the data.\n",
        "\n",
        "19.How would you interpret a large standard error for a regression coefficient? A large standard error indicates that there is a high level of uncertainty about the estimate of the coefficient. This may suggest that the predictor variable does not have a strong relationship with the dependent variable or that there is multicollinearity present.\n",
        "\n",
        "20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?  \n",
        "Heteroscedasticity can be identified by plotting the residuals against the predicted values or independent variables. If the spread of the residuals increases or decreases with the predicted values, it indicates heteroscedasticity. Addressing it is important because it can lead to inefficient estimates and affect the validity of statistical tests.\n",
        "\n",
        "21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        " A high R² with a low adjusted R² suggests that the model may be overfitting the data, meaning it captures noise rather than the underlying relationship. The adjusted R² penalizes the addition of unnecessary predictors, providing a more accurate measure of model performance.\n",
        "\n",
        "22.Why is it important to scale variables in Multiple Linear Regression? Scaling variables is important to ensure that all predictors contribute equally to the model, especially when they are measured on different scales. This can improve the convergence of optimization algorithms and the interpretability of coefficients.\n",
        "\n",
        "23.What is polynomial regression?\n",
        " Polynomial regression is a form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial.\n",
        "\n",
        "24.How does polynomial regression differ from linear regression?\n",
        "Polynomial regression allows for a nonlinear relationship between the independent and dependent variables by including polynomial terms, while linear regression assumes a straight-line relationship.\n",
        "\n",
        "25.When is polynomial regression used?  \n",
        "Polynomial regression is used when the relationship between the independent and dependent variables is curvilinear, meaning that a straight line does not adequately capture the trend in the data.\n",
        "\n",
        "26.What is the general equation for polynomial regression?\n",
        " The general equation for polynomial regression of degree ( n ) is: [ Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\ldots + b_nX^n ] where ( b_0, b_1, \\ldots, b_n ) are the coefficients.\n",
        "\n",
        "27.Can polynomial regression be applied to multiple variables?\n",
        " Yes, polynomial regression can be extended to multiple variables by including polynomial terms for each independent variable and their interactions.\n",
        "\n",
        "28.What are the limitations of polynomial regression?\n",
        " Polynomial regression can lead to overfitting, especially with high-degree polynomials. It can also produce erratic predictions outside the range of the data and may be sensitive to outliers.\n",
        "\n",
        "29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        " Methods include cross-validation, examining residual plots, and using information criteria such as AIC or BIC to balance model fit and complexity.\n",
        "\n",
        "30.Why is visualization important in polynomial regression?\n",
        " Visualization helps to understand the relationship between variables, assess the fit of the model, and identify potential issues such as overfitting or the presence of outliers.\n",
        "\n",
        "31.How is polynomial regression implemented in Python?\n",
        " Polynomial regression can be implemented in Python using libraries such as NumPy and scikit-learn. The PolynomialFeatures class from scikit-learn can be used to create polynomial features, which can then be fitted using a linear regression model."
      ],
      "metadata": {
        "id": "-WZLCpB2sao0"
      }
    }
  ]
}