{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        "\n",
        "Yes, Bagging (Bootstrap Aggregating) can be used for regression problems. In this context, multiple regression models are trained on different subsets of the training data, and their predictions are averaged to produce a final prediction. This helps to reduce variance and improve the model's robustness.\n",
        "\n",
        "2. What is the difference between multiple model training and single model training?\n",
        "\n",
        "Single Model Training: Involves training one model on the entire dataset. The model learns from all available data points, which can lead to overfitting if the model is too complex.\n",
        "Multiple Model Training: Involves training multiple models, often on different subsets of the data (as in Bagging) or using different algorithms (as in ensemble methods). This approach can improve generalization by combining the strengths of different models and reducing the risk of overfitting.\n",
        "\n",
        "3. Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "In Random Forest, feature randomness refers to the practice of selecting a random subset of features for each split in the decision trees. Instead of considering all features, each tree in the forest is trained using a random subset of features. This helps to decorrelate the trees, making the ensemble more robust and reducing overfitting.\n",
        "\n",
        "4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "The Out-of-Bag (OOB) Score is an internal validation method used in Bagging and Random Forests. When training each tree, about one-third of the data is not used (out-of-bag samples). The OOB score is calculated by evaluating the model on these out-of-bag samples, providing an unbiased estimate of the model's performance without the need for a separate validation set.\n",
        "\n",
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "Feature importance in a Random Forest model can be measured using:\n",
        "\n",
        "Mean Decrease Impurity (MDI): This method calculates the total reduction in impurity (e.g., Gini impurity or entropy) brought by each feature across all trees in the forest.\n",
        "Mean Decrease Accuracy (MDA): This method evaluates the decrease in model accuracy when the values of a feature are permuted, indicating how much the model relies on that feature for making predictions.\n",
        "\n",
        "6. Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "A Bagging Classifier works by:\n",
        "\n",
        "Bootstrap Sampling: Creating multiple subsets of the training data by randomly sampling with replacement.\n",
        "Model Training: Training a separate model (e.g., decision tree) on each of these subsets.\n",
        "Aggregation: For classification tasks, the final prediction is made by majority voting among the predictions of all models. For regression tasks, the predictions are averaged.\n",
        "\n",
        "7. How do you evaluate a Bagging Classifierâ€™s performance?\n",
        "\n",
        "The performance of a Bagging Classifier can be evaluated using metrics such as:\n",
        "\n",
        "Accuracy: The proportion of correctly classified instances.\n",
        "Precision, Recall, F1-Score: For classification tasks, these metrics provide insights into the model's performance on different classes.\n",
        "Mean Squared Error (MSE): For regression tasks, MSE can be used to evaluate the average squared difference between predicted and actual values.\n",
        "Cross-Validation: Using k-fold cross-validation to assess the model's performance on different subsets of the data.\n",
        "\n",
        "8. How does a Bagging Regressor work?\n",
        "\n",
        "A Bagging Regressor works similarly to a Bagging Classifier but is used for regression tasks. It involves:\n",
        "\n",
        "Bootstrap Sampling: Creating multiple subsets of the training data.\n",
        "Model Training: Training a separate regression model (e.g., decision tree regressor) on each subset.\n",
        "Aggregation: The final prediction is made by averaging the predictions from all the individual models.\n",
        "\n",
        "9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "The main advantage of ensemble techniques is that they combine multiple models to improve overall performance. By aggregating the predictions of several models, ensemble methods can reduce variance (Bagging), bias (Boosting), or improve predictions (Stacking), leading to better generalization on unseen data.\n",
        "\n",
        "10. What is the main challenge of ensemble methods?\n",
        "\n",
        "The main challenge of ensemble methods is the increased complexity and computational cost. Training multiple models can be resource-intensive, and managing the ensemble can complicate the model deployment and interpretation. Additionally, if the individual models are not diverse enough, the ensemble may not provide significant improvements.\n",
        "\n",
        "11. Explain the key idea behind ensemble techniques.\n",
        "\n",
        "The key idea behind ensemble techniques is to leverage the strengths of multiple models to achieve better predictive performance than any single model could provide. By combining different models, either through averaging (in regression) or voting (in classification), ensemble methods can reduce errors and improve robustness.\n",
        "\n",
        "12. What is a Random Forest Classifier?\n",
        "\n",
        "A Random Forest Classifier is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of the classes (for classification) or the mean prediction (for regression) of the individual trees. It uses bootstrap sampling and feature randomness to create diverse trees, which helps improve accuracy and control overfitting.\n",
        "\n",
        "13. What are the main types of ensemble techniques?\n",
        "\n",
        "The main types of ensemble techniques include:\n",
        "\n",
        "Bagging: Combines predictions from multiple models trained on different subsets of the data (e.g., Random Forest).\n",
        "Boosting: Sequentially trains models, where each model focuses on correcting the errors of the previous ones (e.g., AdaBoost, Gradient Boosting).\n",
        "Stacking: Combines multiple models (base learners) and uses another model (meta-learner) to make the final prediction based on the outputs of the base learners.\n",
        "\n",
        "14. What is ensemble learning in machine learning?\n",
        "\n",
        "Ensemble learning is a machine learning paradigm that combines multiple models to improve overall performance. The idea is that by aggregating the predictions of several models, the ensemble can achieve better accuracy, robustness, and generalization than any individual model.\n",
        "\n",
        "15. When should we avoid using ensemble methods?\n",
        "\n",
        "Ensemble methods should be avoided when:\n",
        "\n",
        "The dataset is very small, as the complexity of ensemble methods may lead to overfitting.\n",
        "The individual models are not diverse enough, which may not provide significant improvements.\n",
        "Computational resources are limited, as ensemble methods can be resource-intensive.\n",
        "\n",
        "16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "Bagging helps reduce overfitting by averaging the predictions of multiple models trained on different subsets of the data. This averaging process smooths out the predictions and reduces the variance of the model. By training on different samples, Bagging also helps to ensure that the model does not become too sensitive to noise in the training data.\n",
        "\n",
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "Random Forest is generally better than a single Decision Tree because:\n",
        "\n",
        "It reduces overfitting by averaging the predictions of multiple trees, leading to better generalization on unseen data.\n",
        "It introduces randomness in both data sampling and feature selection, which helps create diverse trees that capture different patterns in the data.\n",
        "It is more robust to noise and outliers compared to a single Decision Tree, which can be overly sensitive to such variations.\n",
        "\n",
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "Bootstrap sampling is a technique used in Bagging to create multiple subsets of the training data by randomly sampling with replacement. Each model in the ensemble is trained on a different bootstrap sample, which introduces diversity among the models. This diversity helps to reduce variance and improve the overall performance of the ensemble.\n",
        "\n",
        "19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "Some real-world applications of ensemble techniques include:\n",
        "\n",
        "Finance: Credit scoring and risk assessment.\n",
        "Healthcare: Disease diagnosis and prediction.\n",
        "Marketing: Customer segmentation and targeting.\n",
        "Image Recognition: Object detection and classification.\n",
        "Natural Language Processing: Sentiment analysis and spam detection.\n",
        "\n",
        "20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Bagging (Bootstrap Aggregating) trains multiple models independently on different subsets of the data and combines their predictions (averaging for regression, voting for classification). It aims to reduce variance and improve stability.\n",
        "\n",
        "Boosting trains models sequentially, where each new model focuses on correcting the errors made by the previous models. It combines their predictions in a weighted manner. Boosting aims to reduce bias and improve accuracy, often leading to a stronger model than Bagging."
      ],
      "metadata": {
        "id": "fbmC8MfUAj7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ],
      "metadata": {
        "id": "HqreDmrLCh4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "# The 'base_estimator' argument has been replaced with 'estimator' in newer versions of scikit-learn.\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyGsmoT3Cx6e",
        "outputId": "5c6c3126-5f81-4580-827c-0f144fa4bf6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "gT_d8LZhC1g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Fetch the California housing dataset instead of the Boston dataset\n",
        "# This addresses the ethical concerns and uses a suitable alternative.\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor using Decision Trees\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50)\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate MSE\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJhd6VmSDSmi",
        "outputId": "e6f9336e-3f38-43dd-94e7-a50387b7a2ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "yhgoZJeMDc23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "for i, score in enumerate(feature_importances):\n",
        "    print(f'Feature {cancer.feature_names[i]}: {score:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKm_eTp5DY68",
        "outputId": "e02ce606-49d7-4b26-abbd-298166027f91"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature mean radius: 0.0513\n",
            "Feature mean texture: 0.0140\n",
            "Feature mean perimeter: 0.0372\n",
            "Feature mean area: 0.0462\n",
            "Feature mean smoothness: 0.0066\n",
            "Feature mean compactness: 0.0175\n",
            "Feature mean concavity: 0.0407\n",
            "Feature mean concave points: 0.1221\n",
            "Feature mean symmetry: 0.0053\n",
            "Feature mean fractal dimension: 0.0044\n",
            "Feature radius error: 0.0258\n",
            "Feature texture error: 0.0060\n",
            "Feature perimeter error: 0.0219\n",
            "Feature area error: 0.0111\n",
            "Feature smoothness error: 0.0048\n",
            "Feature compactness error: 0.0060\n",
            "Feature concavity error: 0.0086\n",
            "Feature concave points error: 0.0052\n",
            "Feature symmetry error: 0.0047\n",
            "Feature fractal dimension error: 0.0058\n",
            "Feature worst radius: 0.1212\n",
            "Feature worst texture: 0.0175\n",
            "Feature worst perimeter: 0.1204\n",
            "Feature worst area: 0.1076\n",
            "Feature worst smoothness: 0.0134\n",
            "Feature worst compactness: 0.0043\n",
            "Feature worst concavity: 0.0302\n",
            "Feature worst concave points: 0.1149\n",
            "Feature worst symmetry: 0.0177\n",
            "Feature worst fractal dimension: 0.0076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ],
      "metadata": {
        "id": "Pc_CJkUeDjz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing # Instead of load_boston, use fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset instead of the Boston dataset\n",
        "# This addresses the ethical concerns and uses a suitable alternative.\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Train a single Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor()\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate MSE for Random Forest\n",
        "y_pred_rf = rf_regressor.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Predict and evaluate MSE for Decision Tree\n",
        "y_pred_dt = dt_regressor.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Compare MSE\n",
        "print(f'Mean Squared Error (Random Forest): {mse_rf:.2f}')\n",
        "print(f'Mean Squared Error (Decision Tree): {mse_dt:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0ADUhtwDwFy",
        "outputId": "6c21f5ef-0916-4383-d9c3-23fbf6259382"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Random Forest): 0.25\n",
            "Mean Squared Error (Decision Tree): 0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ],
      "metadata": {
        "id": "AmGLm84GD79I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier using SVM\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_svm = BaggingClassifier(estimator=SVC(), n_estimators=50)\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy (SVM): {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyOlOldnEGlT",
        "outputId": "b87ea052-414f-4755-c1f8-a48caace1d57"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (SVM): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.Train a Random Forest Classifier with different numbers of trees and compare accuracy"
      ],
      "metadata": {
        "id": "kdK3-Q5uEKC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifiers with different numbers of trees\n",
        "for n_trees in [10, 50, 100, 200]:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=n_trees)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'Accuracy with {n_trees} trees: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__qDNU7bENGt",
        "outputId": "5f7a3bb8-855e-4eda-ae1a-1a5a24abffaa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 10 trees: 0.95\n",
            "Accuracy with 50 trees: 0.96\n",
            "Accuracy with 100 trees: 0.96\n",
            "Accuracy with 200 trees: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ],
      "metadata": {
        "id": "PiZ0HAQwEQDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier using Logistic Regression\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_logistic = BaggingClassifier(estimator=LogisticRegression(), n_estimators=50)  # Change is here\n",
        "bagging_logistic.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate AUC score\n",
        "y_pred_proba = bagging_logistic.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f'AUC Score: {auc_score:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBG0VKVKEa9R",
        "outputId": "0a27c68d-47b3-4b17-e029-6e291768bb85"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 27. Train a Random Forest Regressor and analyze feature importance scores"
      ],
      "metadata": {
        "id": "srhHovrnEew-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing # Instead of load_boston, use fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load the California housing dataset instead of the Boston dataset\n",
        "# This addresses the ethical concerns and uses a suitable alternative.\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Analyze feature importance scores\n",
        "feature_importances = rf_regressor.feature_importances_\n",
        "for i, score in enumerate(feature_importances):\n",
        "    # Access feature names from the dataset\n",
        "    print(f'Feature {housing.feature_names[i]}: {score:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV6WwGwcErMo",
        "outputId": "8977ea8b-880e-4e42-c2de-b2313841ac51"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature MedInc: 0.5234\n",
            "Feature HouseAge: 0.0543\n",
            "Feature AveRooms: 0.0421\n",
            "Feature AveBedrms: 0.0292\n",
            "Feature Population: 0.0311\n",
            "Feature AveOccup: 0.1387\n",
            "Feature Latitude: 0.0915\n",
            "Feature Longitude: 0.0897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "nyaidVcxEvPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "bagging_model = BaggingClassifier(estimator=RandomForestClassifier(n_estimators=10), n_estimators=50) # Replace 'base_estimator' with 'estimator'\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f'Accuracy of Bagging Classifier: {accuracy_bagging:.2f}')\n",
        "print(f'Accuracy of Random Forest Classifier: {accuracy_rf:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AKvAKdmExVR",
        "outputId": "16e16511-5493-4937-b76d-14f7ab778e51"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Bagging Classifier: 0.96\n",
            "Accuracy of Random Forest Classifier: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "pVnVYv4oE1E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Cross-Validation Score: {grid_search.best_score_:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40r2V0JdE_Q2",
        "outputId": "dd273f54-a5ab-48a8-c62b-f2dc1a3236a2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Best Cross-Validation Score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "60Nr178IFMFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Cross-Validation Score: {grid_search.best_score_:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co7gEEYBFOwY",
        "outputId": "108c7cf7-8f91-4344-d4d6-c94f17d7f3d9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "Best Cross-Validation Score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ],
      "metadata": {
        "id": "38zrxO-oFdMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset instead of the Boston dataset\n",
        "# This addresses the ethical concerns and uses a suitable alternative.\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compare performance with different numbers of base estimators\n",
        "for n_estimators in [10, 50, 100]:\n",
        "    # Replace 'base_estimator' with 'estimator'\n",
        "    bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f'Mean Squared Error with {n_estimators} estimators: {mse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Km_xCFYF1hJ",
        "outputId": "ac6639af-6609-4f6f-b03a-6a1730a6d258"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error with 10 estimators: 0.28\n",
            "Mean Squared Error with 50 estimators: 0.26\n",
            "Mean Squared Error with 100 estimators: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Random Forest Classifier and analyze misclassified samples\n"
      ],
      "metadata": {
        "id": "mSzD7tWyGGQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified = X_test[y_pred != y_test]\n",
        "print(f'Misclassified Samples: {misclassified}')\n",
        " ###  Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "# Train a Bagging Classifier\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "dt_model = DecisionTreeClassifier()\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy for Bagging Classifier\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Predict and evaluate accuracy for Decision Tree Classifier\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f'Accuracy of Bagging Classifier: {accuracy_bagging:.2f}')\n",
        "print(f'Accuracy of Decision Tree Classifier: {accuracy_dt:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C17VEqQhGKJU",
        "outputId": "81db5f7a-6118-4bc8-b826-a0a84c72bb05"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.96\n",
            "Misclassified Samples: [[1.334e+01 1.586e+01 8.649e+01 5.200e+02 1.078e-01 1.535e-01 1.169e-01\n",
            "  6.987e-02 1.942e-01 6.902e-02 2.860e-01 1.016e+00 1.535e+00 1.296e+01\n",
            "  6.794e-03 3.575e-02 3.980e-02 1.383e-02 2.134e-02 4.603e-03 1.553e+01\n",
            "  2.319e+01 9.666e+01 6.149e+02 1.536e-01 4.791e-01 4.858e-01 1.708e-01\n",
            "  3.527e-01 1.016e-01]\n",
            " [1.380e+01 1.579e+01 9.043e+01 5.841e+02 1.007e-01 1.280e-01 7.789e-02\n",
            "  5.069e-02 1.662e-01 6.566e-02 2.787e-01 6.205e-01 1.957e+00 2.335e+01\n",
            "  4.717e-03 2.065e-02 1.759e-02 9.206e-03 1.220e-02 3.130e-03 1.657e+01\n",
            "  2.086e+01 1.103e+02 8.124e+02 1.411e-01 3.542e-01 2.779e-01 1.383e-01\n",
            "  2.589e-01 1.030e-01]\n",
            " [1.396e+01 1.705e+01 9.143e+01 6.024e+02 1.096e-01 1.279e-01 9.789e-02\n",
            "  5.246e-02 1.908e-01 6.130e-02 4.250e-01 8.098e-01 2.563e+00 3.574e+01\n",
            "  6.351e-03 2.679e-02 3.119e-02 1.342e-02 2.062e-02 2.695e-03 1.639e+01\n",
            "  2.207e+01 1.081e+02 8.260e+02 1.512e-01 3.262e-01 3.209e-01 1.374e-01\n",
            "  3.068e-01 7.957e-02]\n",
            " [1.448e+01 2.146e+01 9.425e+01 6.482e+02 9.444e-02 9.947e-02 1.204e-01\n",
            "  4.938e-02 2.075e-01 5.636e-02 4.204e-01 2.220e+00 3.301e+00 3.887e+01\n",
            "  9.369e-03 2.983e-02 5.371e-02 1.761e-02 2.418e-02 3.249e-03 1.621e+01\n",
            "  2.925e+01 1.084e+02 8.089e+02 1.306e-01 1.976e-01 3.349e-01 1.225e-01\n",
            "  3.020e-01 6.846e-02]]\n",
            "Accuracy of Bagging Classifier: 1.00\n",
            "Accuracy of Decision Tree Classifier: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 33. Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "JZyKAOQxGdUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cancer.target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "VdNdq74-GkAI",
        "outputId": "252105bb-cfbd-4d81-bf20-dc51405f87d9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "ZQfWZMOpGntt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('decision_tree', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True)),\n",
        "    ('logistic_regression', LogisticRegression())\n",
        "]\n",
        "\n",
        "# Create a Stacking Classifier\n",
        "stacking_model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Stacking Classifier: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgN3UOwVGrNd",
        "outputId": "61e450b3-0c60-40ce-ad58-da5d7acce815"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Stacking Classifier: 0.96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "QehowjTiGvGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances and sort them\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "indices = feature_importances.argsort()[::-1]\n",
        "\n",
        "# Print the top 5 most important features\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances and sort them\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "indices = feature_importances.argsort()[::-1]\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for i in range(5):\n",
        "    print(f\"{cancer.feature_names[indices[i]]}: {feature_importances[indices[i]]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSKfm5cIGyeZ",
        "outputId": "b767940f-844b-455e-9dc0-43ce584207b0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "worst concave points: 0.1488\n",
            "worst radius: 0.1307\n",
            "worst perimeter: 0.1184\n",
            "mean concave points: 0.1091\n",
            "worst area: 0.0902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "ES8muKtJGxyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WylttzgqG8VM",
        "outputId": "b90ad965-3ad4-4997-d344-fe30ed9155fd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.96\n",
            "Recall: 0.97\n",
            "F1 Score: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "xKDQaDIEHOiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Analyze the effect of max_depth on accuracy\n",
        "for max_depth in [None, 5, 10, 15, 20]:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=max_depth)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'Accuracy with max_depth={max_depth}: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEb2VvUXHRRY",
        "outputId": "9a2f8ce4-cde5-43be-aae5-15640ac13fd7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=None: 0.96\n",
            "Accuracy with max_depth=5: 0.96\n",
            "Accuracy with max_depth=10: 0.96\n",
            "Accuracy with max_depth=15: 0.96\n",
            "Accuracy with max_depth=20: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance"
      ],
      "metadata": {
        "id": "rbBZSwLAHT_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing # Instead of load_boston, use fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset instead of the Boston dataset\n",
        "# This addresses the ethical concerns and uses a suitable alternative.\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compare performance with different base estimators\n",
        "for base_estimator in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
        "    # Replace 'base_estimator' with 'estimator'\n",
        "    bagging_regressor = BaggingRegressor(estimator=base_estimator, n_estimators=50)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f'Mean Squared Error with {base_estimator.__class__.__name__}: {mse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLwtobhSHqnb",
        "outputId": "e8caa4aa-9571-434f-f53e-1c9f9755fbd3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error with DecisionTreeRegressor: 0.26\n",
            "Mean Squared Error with KNeighborsRegressor: 1.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ],
      "metadata": {
        "id": "HLqBYJLDHuSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and evaluate ROC-AUC Score\n",
        "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f'ROC-AUC Score: {roc_auc:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkrX7F3lHxAV",
        "outputId": "061b045d-777b-4345-8b52-f5a09ef1d4f1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Classifier and evaluate its performance using cross-validation"
      ],
      "metadata": {
        "id": "EGJTI0YUH0Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Instead of importing 'cross_val', import 'cross_val_score' or 'cross_validate'\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score  # Import cross_val_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50)  # Change is here\n",
        "\n",
        "# Evaluate performance using cross-validation\n",
        "# Use cross_val_score for evaluation\n",
        "cv_scores = cross_val_score(bagging_model, X_train, y_train, cv=5)\n",
        "print(f'Cross-Validation Scores: {cv_scores}')\n",
        "print(f'Mean Cross-Validation Score: {cv_scores.mean():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_3snZ8eIITQ",
        "outputId": "07b21cf2-fa49-4697-a178-95c26c69c851"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [0.95604396 0.92307692 0.97802198 0.95604396 0.93406593]\n",
            "Mean Cross-Validation Score: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "i9GblBGRH-dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1G2-aj2QIKy3",
        "outputId": "6bb61b9b-0aac-4461-a3aa-860390903c04"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ],
      "metadata": {
        "id": "IftXddDgIOHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('random_forest', RandomForestClassifier(n_estimators=50)),\n",
        "    ('logistic_regression', LogisticRegression())\n",
        "]\n",
        "\n",
        "# Create a Stacking Classifier\n",
        "stacking_model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Stacking Classifier: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gkiQKuIIRvc",
        "outputId": "f4509212-3759-4fd0-d160-b5c5a4eeedf4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Stacking Classifier: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "2a-8FMGvIVAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of importing load_boston, import fetch_california_housing\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset instead of the Boston dataset\n",
        "# This addresses the ethical concerns and uses a suitable alternative.\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compare performance with different levels of bootstrap samples\n",
        "for bootstrap in [True, False]:\n",
        "    # Replace 'base_estimator' with 'estimator' in newer versions of scikit-learn\n",
        "    bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, bootstrap=bootstrap)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f'Mean Squared Error with bootstrap={bootstrap}: {mse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldaw3LN9Ijqg",
        "outputId": "ba785f05-8009-4405-b5c7-f40927c7d274"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error with bootstrap=True: 0.26\n",
            "Mean Squared Error with bootstrap=False: 0.46\n"
          ]
        }
      ]
    }
  ]
}